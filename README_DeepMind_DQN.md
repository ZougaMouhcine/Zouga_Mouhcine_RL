# DQN avec Architecture DeepMind pour Goal Dynamique

## Vue d'ensemble

Ce projet impl√©mente et compare plusieurs approches d'apprentissage par renforcement pour un environnement Grid World, avec un focus particulier sur un **DQN avec architecture DeepMind (convolutionnelle)** pour g√©rer des goals dynamiques.

## Impl√©mentations

### 1. **DeepMind DQN (Architecture CNN) - Goal Dynamique** üß†
- **Architecture**: Couches convolutionnelles (3 canaux ‚Üí 32 ‚Üí 64 ‚Üí 64 ‚Üí FC)
- **Environnement**: Grille 6x6 avec goal changeant √† chaque √©pisode
- **Repr√©sentation**: Grille 3D (canal agent, canal goal, canal obstacles)
- **Param√®tres**: ~100K
- **Avantages**: 
  - Capture la structure spatiale de l'environnement
  - G√©n√©ralise bien √† diff√©rentes positions de goal
  - Exploite les couches convolutionnelles pour d√©tecter des patterns spatiaux

### 2. **Q-Learning Tabulaire - Goal Dynamique** üìä
- **Architecture**: Table Q traditionnelle
- **Environnement**: Grille 6x6 avec goal changeant
- **Param√®tres**: 144 valeurs (36 √©tats √ó 4 actions)
- **Avantages**: Simple, convergence rapide pour petits espaces

### 3. **DQN Standard (Fully-Connected) - Goal Fixe (Baseline)** ü§ñ
- **Architecture**: R√©seaux fully-connected (2 ‚Üí 128 ‚Üí 128 ‚Üí 64 ‚Üí 4)
- **Environnement**: Grille 7x7 avec goal fixe
- **Param√®tres**: ~17K
- **Baseline**: Pour comparaison avec les m√©thodes √† goal dynamique

### 4. **Q-Learning Tabulaire - Goal Fixe (Baseline Optimal)** ‚úÖ
- **Architecture**: Table Q
- **Environnement**: Grille 7x7 avec goal fixe
- **Param√®tres**: 196 valeurs (49 √©tats √ó 4 actions)
- **Baseline optimale**: Solution optimale garantie

## Architecture DeepMind DQN

```
Input: Grille 3D (3, 6, 6)
  ‚Üì
Conv1: 3 ‚Üí 32 (kernel 3√ó3, stride 1, padding 1) + ReLU
  ‚Üì
Conv2: 32 ‚Üí 64 (kernel 3√ó3, stride 1, padding 1) + ReLU
  ‚Üì
Conv3: 64 ‚Üí 64 (kernel 2√ó2, stride 1, padding 0) + ReLU
  ‚Üì
Flatten
  ‚Üì
FC1: flattened ‚Üí 512 + ReLU
  ‚Üì
FC2: 512 ‚Üí 4 (Q-values pour chaque action)
```

### Repr√©sentation de l'√âtat (3 Canaux)

- **Canal 0**: Position de l'agent (1 √† la position de l'agent, 0 ailleurs)
- **Canal 1**: Position du goal (1 √† la position du goal, 0 ailleurs)
- **Canal 2**: Obstacles (1 aux positions d'obstacles, 0 ailleurs)

## R√©sultats Exp√©rimentaux

| M√©thode | Environnement | R√©compense | Pas Moyens | Taux Succ√®s | Param√®tres |
|---------|--------------|------------|------------|-------------|------------|
| DeepMind DQN (CNN) | Dynamic 6√ó6 | ~XX.XX | ~XX.X | ~XX% | ~100K |
| Q-Learning Tab. | Dynamic 6√ó6 | ~XX.XX | ~XX.X | ~XX% | 144 |
| DQN Standard (FC) | Fixed 7√ó7 | ~XX.XX | ~XX.X | ~XX% | ~17K |
| Q-Learning Tab. | Fixed 7√ó7 | ~XX.XX | ~XX.X | ~XX% | 196 |

*(Les valeurs exactes seront disponibles apr√®s l'ex√©cution du notebook)*

## Observations Cl√©s

### Goal Dynamique vs Goal Fixe
- Le goal dynamique est significativement plus difficile
- Les agents doivent apprendre une politique plus g√©n√©rale
- R√©compenses g√©n√©ralement plus faibles avec goal dynamique

### Architecture DeepMind (CNN)
**Avantages:**
- ‚úÖ Capture efficacement la structure spatiale de la grille
- ‚úÖ G√©n√©ralise √† diff√©rentes positions de goal et obstacles
- ‚úÖ Performance comp√©titive avec Q-Learning tabulaire

**Inconv√©nients:**
- ‚ùå Plus complexe et n√©cessite plus de temps d'entra√Ænement
- ‚ùå Beaucoup plus de param√®tres (~100K vs 144)
- ‚ùå N√©cessite plus de m√©moire et de calcul

### Q-Learning Tabulaire
**Avantages:**
- ‚úÖ Excellent pour petits espaces d'√©tats (< 1000 √©tats)
- ‚úÖ Convergence rapide et stable
- ‚úÖ Solution optimale garantie avec exploration suffisante

**Inconv√©nients:**
- ‚ùå Ne passe pas √† l'√©chelle (m√©moire O(|S| √ó |A|))
- ‚ùå Impossible pour √©tats continus

## Recommandations

| Sc√©nario | M√©thode Recommand√©e |
|----------|---------------------|
| Petit espace d'√©tats (<1000) | Q-Learning Tabulaire |
| Structure spatiale importante | DeepMind DQN (CNN) |
| Grand espace d'√©tats sans structure spatiale | DQN Standard (FC) |
| Goal/Environnement dynamique | DeepMind DQN (CNN) + repr√©sentation riche |

## Fichiers G√©n√©r√©s

- `q_values_5x5.csv`, `q_values_7x7.csv`, `q_values_10x10.csv` - Q-values pour grilles fixes
- `q_values_dynamic_goal_6x6.csv` - Q-values pour goal dynamique
- `dqn_model_7x7.pth` - Mod√®le DQN standard (baseline)
- `dqn_training_stats_7x7.csv` - Statistiques d'entra√Ænement DQN standard
- `deepmind_dqn_model_6x6_dynamic.pth` - Mod√®le DeepMind DQN
- `deepmind_dqn_training_stats_6x6_dynamic.csv` - Statistiques d'entra√Ænement DeepMind DQN

## Utilisation

1. Ouvrir le notebook `Q_Learning_Grid_World.ipynb`
2. Ex√©cuter les cellules s√©quentiellement
3. Les sections 30.* contiennent l'impl√©mentation DeepMind DQN
4. La section 31 pr√©sente la comparaison finale et les conclusions

## D√©pendances

```python
numpy
matplotlib
pandas
torch
```

## Auteur

Zouga Mouhcine

## Date

Octobre 2025
